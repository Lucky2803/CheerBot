{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install openai==0.28"
      ],
      "metadata": {
        "id": "9WMwiO29lbAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3_qb-X8lYSj"
      },
      "outputs": [],
      "source": [
        "# # import openai\n",
        "\n",
        "# # openai.api_key = \"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\"\n",
        "\n",
        "\n",
        "# # def generate_response(prompt, emotion):\n",
        "# #     # Define the API parameters\n",
        "# #     model_engine = \"text-davinci-002\"\n",
        "# #     prompt = f\"You say: {prompt}\\nBot responds: \"\n",
        "# #     temperature = 0.9\n",
        "# #     max_tokens = 60\n",
        "\n",
        "# #     # Call the API to generate the response\n",
        "# #     response = openai.Completion.create(\n",
        "# #         engine=model_engine,\n",
        "# #         prompt=prompt,\n",
        "# #         temperature=temperature,\n",
        "# #         max_tokens=max_tokens,\n",
        "# #     )\n",
        "\n",
        "# #     # Return the generated response\n",
        "# #     return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "# #new key: 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\n",
        "# # openai.api_key = \"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\"\n",
        "\n",
        "# import openai\n",
        "# import nltk\n",
        "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# openai.api_key = \"\"\n",
        "\n",
        "# def generate_response(sentence, emotion):\n",
        "#     # Define the API parameters\n",
        "#     model_engine = \"text-davinci-002\"\n",
        "#     temperature = 0.9\n",
        "#     max_tokens = 60\n",
        "\n",
        "#     # Construct the prompt based on the user's input and emotion\n",
        "#     if emotion == \"happy\":\n",
        "#         prompt = f\"You said: {sentence}\\nBot responds with a {emotion} tone:\"\n",
        "#     elif emotion in [\"sad\", \"anger\", \"fear\"]:\n",
        "#         prompt = f\"You said: {sentence}\\nTo address your {emotion} emotion, Bot suggests:\"\n",
        "#     elif emotion == \"surprise\":\n",
        "#         # Check the sentiment of the user's input\n",
        "#         sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "#         sentiment = sentiment_analyzer.polarity_scores(sentence)[\"compound\"]\n",
        "#         if sentiment > 0:\n",
        "#             prompt = f\"You said: {sentence}\\nBot responds with a {emotion} tone: Please tell me more about that.\"\n",
        "#         elif sentiment < 0:\n",
        "#             prompt = f\"You said: {sentence}\\nTo address the {emotion} emotion, Bot suggests:\"\n",
        "#         else:\n",
        "#             prompt = f\"You said: {sentence}\\nBot responds: That's surprising. Can you tell me more?\"\n",
        "\n",
        "#     # Call the API to generate the response\n",
        "#     response = openai.Completion.create(\n",
        "#         engine=model_engine,\n",
        "#         prompt=prompt,\n",
        "#         temperature=temperature,\n",
        "#         max_tokens=max_tokens,\n",
        "#     )\n",
        "\n",
        "#     # Return the generated response\n",
        "#     return response.choices[0].text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "# import nltk\n",
        "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# openai.api_key = \"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\"\n",
        "\n",
        "# def generate_response(sentence, emotion):\n",
        "#     # Define the API parameters\n",
        "#     model_engine = \"text-davinci-002\"\n",
        "#     temperature = 0.9\n",
        "#     max_tokens = 60\n",
        "\n",
        "#     # Construct the prompt based on the user's input and emotion\n",
        "#     if emotion == \"happy\":\n",
        "#         prompt = f\"You said: {sentence}\\nBot responds with a {emotion} tone:\"\n",
        "#     elif emotion in [\"sad\", \"anger\", \"fear\"]:\n",
        "#         prompt = f\"You said: {sentence}\\nTo address your {emotion} emotion, Bot suggests:\"\n",
        "#     elif emotion == \"surprise\":\n",
        "#         # Check the sentiment of the user's input\n",
        "#         sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "#         sentiment = sentiment_analyzer.polarity_scores(sentence)[\"compound\"]\n",
        "#         if sentiment > 0:\n",
        "#             prompt = f\"You said: {sentence}\\nBot responds with a {emotion} tone: Please tell me more about that.\"\n",
        "#         elif sentiment < 0:\n",
        "#             prompt = f\"You said: {sentence}\\nTo address the {emotion} emotion, Bot suggests:\"\n",
        "#         else:\n",
        "#             prompt = f\"You said: {sentence}\\nBot responds: That's surprising. Can you tell me more?\"\n",
        "\n",
        "#     # Call the API to generate the response\n",
        "#     response = openai.ChatCompletion.create(\n",
        "#         model=model_engine,\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt}\n",
        "#         ],\n",
        "#         temperature=temperature,\n",
        "#         max_tokens=max_tokens,\n",
        "#     )\n",
        "\n",
        "#     # Return the generated response\n",
        "#     return response['choices'][0]['message']['content'].strip()\n"
      ],
      "metadata": {
        "id": "lNaLC-h4immD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"I am very happy at my professor he told the syallbus properly, and the exam was exactly what i had expected it to be like\"\n",
        "# emotion = \"happy\""
      ],
      "metadata": {
        "id": "nvBuRrCtJltE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_response(text, emotion)"
      ],
      "metadata": {
        "id": "YYeNBynxJn1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai"
      ],
      "metadata": {
        "id": "LcUpECaHpb4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "import google.generativeai as palm"
      ],
      "metadata": {
        "id": "t-4sIhZApcDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key='AIzaSyDQOgAz-lwfJKZguSnLMaw3acl4wxPHRlY')"
      ],
      "metadata": {
        "id": "i0C2tnAJpcFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sukvB2mjpcIM",
        "outputId": "cf4a803e-6132-44df-b1c7-a5cf1e301228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(sentence, emotion, Model):\n",
        "    # Construct the prompt based on the user's input and emotion\n",
        "    if emotion == \"happy\":\n",
        "        prompt = f\"You said: {sentence}\\nBot responds with a {emotion} tone:\"\n",
        "    elif emotion in [\"sad\", \"anger\", \"fear\"]:\n",
        "        prompt = f\"You said: {sentence}\\nTo address your {emotion} emotion, Bot suggests:\"\n",
        "    elif emotion == \"surprise\":\n",
        "        # Check the sentiment of the user's input\n",
        "        sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "        sentiment = sentiment_analyzer.polarity_scores(sentence)[\"compound\"]\n",
        "        if sentiment > 0:\n",
        "            prompt = f\"You said: {sentence}\\nBot responds with a {emotion} tone: Please tell me more about that.\"\n",
        "        elif sentiment < 0:\n",
        "            prompt = f\"You said: {sentence}\\nTo address the {emotion} emotion, Bot suggests:\"\n",
        "        else:\n",
        "            prompt = f\"You said: {sentence}\\nBot responds: That's surprising. Can you tell me more?\"\n",
        "\n",
        "    completion = palm.generate_text(\n",
        "        model=Model,\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        # The maximum length of the response\n",
        "        max_output_tokens=800,\n",
        "    )\n",
        "\n",
        "    return completion.result\n"
      ],
      "metadata": {
        "id": "iPEoABAYpcP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am very happy at my professor he told the syallbus properly, and the exam was exactly what i had expected it to be like\"\n",
        "emotion = \"happy\""
      ],
      "metadata": {
        "id": "esJXD-Z3qOdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(text, emotion, 'models/text-bison-001')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4cjA7VMDrS_1",
        "outputId": "f2f1790c-75f9-4386-dde5-da6e0082bd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm glad to hear that you're happy with your professor. It sounds like they're doing a great job of communicating with students and making sure that the exams are fair.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}