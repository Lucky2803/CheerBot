{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "eepGikkBqDig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set up the API key\n",
        "openai.api_key = \"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\"\n"
      ],
      "metadata": {
        "id": "bxxxXIsDxR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autocorrect(sentence):\n",
        "    # Define the prompt and parameters\n",
        "    prompt = \"Please correct the following sentence for grammar errors: \" + sentence\n",
        "    model = \"text-davinci-002\"\n",
        "    temperature = 0.6\n",
        "    max_tokens = 60\n",
        "\n",
        "    # Call the API to generate the response\n",
        "    response = openai.Completion.create(\n",
        "        engine=model,\n",
        "        prompt=prompt,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    # Extract the corrected sentence from the response\n",
        "    corrected_sentence = response.choices[0].text.strip()\n",
        "\n",
        "    # Return the corrected sentence\n",
        "    return corrected_sentence\n"
      ],
      "metadata": {
        "id": "bu9x5vZ9ptZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set up the API key\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Call the models method to get the list of models\n",
        "models = openai.Model.list()\n",
        "\n",
        "# Print the name and ID of each model\n",
        "for model in models[\"data\"]:\n",
        "    print(model[\"id\"])\n"
      ],
      "metadata": {
        "id": "raBwCVWxxUbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmz3PUuxiwv6",
        "outputId": "fd6551ad-9a93-4a26-a286-d4c729af37ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 KB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# # Install PySpark and Spark NLP\n",
        "# ! pip install -q pyspark==3.3.0 spark-nlp==4.2.8\n",
        "# import json\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# import sparknlp\n",
        "# import pyspark.sql.functions as F\n",
        "\n",
        "# from pyspark.ml import Pipeline\n",
        "# from pyspark.sql import SparkSession\n",
        "# from sparknlp.annotator import *\n",
        "# from sparknlp.base import *\n",
        "# from sparknlp.pretrained import PretrainedPipeline\n",
        "# from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "# spark = sparknlp.start()\n",
        "# document_assembler = DocumentAssembler()\\\n",
        "#   .setInputCol(\"text\")\\\n",
        "#   .setOutputCol(\"document\")\n",
        "\n",
        "# tokenizer = RecursiveTokenizer()\\\n",
        "#   .setInputCols([\"document\"])\\\n",
        "#   .setOutputCol(\"token\")\\\n",
        "#   .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
        "#   .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"‘s\"])\n",
        "\n",
        "# spell_model = ContextSpellCheckerModel\\\n",
        "#     .pretrained('spellcheck_dl')\\\n",
        "#     .setInputCols(\"token\")\\\n",
        "#     .setOutputCol(\"corrected\")\n",
        "\n",
        "# finisher = Finisher().setInputCols(\"corrected\")\n",
        "\n",
        "# light_pipeline = Pipeline(stages = [document_assembler,\n",
        "#                                     tokenizer,\n",
        "#                                     spell_model,\n",
        "#                                     finisher])\n",
        "# ## For comparison\n",
        "# full_pipeline = Pipeline(stages = [document_assembler,\n",
        "#                                    tokenizer,\n",
        "#                                    spell_model])\n",
        "\n",
        "# empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "# pipeline_model = full_pipeline.fit(empty_ds)\n",
        "# l_pipeline_model = LightPipeline(light_pipeline.fit(empty_ds))\n",
        "# text = \"hey I feel ellated that I've tooped the class\"\n",
        "# list = []\n",
        "# list.append(text)\n",
        "\n",
        "# list\n",
        "# def Autocorrect(list):\n",
        "#   df = spark.createDataFrame(pd.DataFrame({\"text\": list}))\n",
        "#   result = pipeline_model.transform(df)\n",
        "#   light_result = l_pipeline_model.annotate(list[0])\n",
        "#   result = ' '.join(light_result['corrected'])\n",
        "#   return result\n",
        "# res = Autocorrect(list)\n",
        "# res"
      ]
    }
  ]
}